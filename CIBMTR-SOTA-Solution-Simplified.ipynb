{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripition\n",
    "\n",
    "Simplified version with only one classifier and one regressor. Total ensemble models take too much time to train & tune.\n",
    "\n",
    " Idea credit to https://kaggle.com/competitions/equity-post-HCT-survival-predictions/writeups/agalar-2nd-place-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-17T18:30:33.248012Z",
     "iopub.status.busy": "2025-12-17T18:30:33.247660Z",
     "iopub.status.idle": "2025-12-17T18:30:58.194313Z",
     "shell.execute_reply": "2025-12-17T18:30:58.193339Z",
     "shell.execute_reply.started": "2025-12-17T18:30:33.247950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd==1.7.0) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd==1.7.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd==1.7.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->autograd==1.7.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->autograd==1.7.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->autograd==1.7.0) (2024.2.0)\n",
      "autograd is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Processing /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: autograd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.7.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd>=1.2.0->autograd-gamma==0.5.0) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2024.2.0)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=8e65520b59eb11280fa994cd4c211f391c483d6599b0ec3a79cfddd2f5b51b1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/b5/e0/4c79e15c0b5f2c15ecf613c720bb20daab20a666eb67135155\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: autograd-gamma\n",
      "Successfully installed autograd-gamma-0.5.0\n",
      "Processing /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
      "Installing collected packages: interface-meta\n",
      "Successfully installed interface-meta-1.3.0\n",
      "Processing /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic==1.0.2) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.5->formulaic==1.0.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.5->formulaic==1.0.2) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.5->formulaic==1.0.2) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.5->formulaic==1.0.2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.5->formulaic==1.0.2) (2024.2.0)\n",
      "Installing collected packages: formulaic\n",
      "Successfully installed formulaic-1.0.2\n",
      "Processing /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (2.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (3.7.5)\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.0.2)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (2.8.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.30.0) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->lifelines==0.30.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->lifelines==0.30.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->lifelines==0.30.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->lifelines==0.30.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->lifelines==0.30.0) (2024.2.0)\n",
      "Installing collected packages: lifelines\n",
      "Successfully installed lifelines-0.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T18:31:43.221731Z",
     "iopub.status.busy": "2025-12-17T18:31:43.221393Z",
     "iopub.status.idle": "2025-12-17T18:31:49.445718Z",
     "shell.execute_reply": "2025-12-17T18:31:49.444677Z",
     "shell.execute_reply.started": "2025-12-17T18:31:43.221689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready to roll!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lifelines.utils import concordance_index\n",
    "from numba import jit\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# --- Configuration ---\n",
    "IS_LOCAL = False\n",
    "\n",
    "if IS_LOCAL:\n",
    "    BASE_DIR = './data/'\n",
    "    WORK_DIR = './working/'\n",
    "else:\n",
    "    BASE_DIR = '/kaggle/input/equity-post-HCT-survival-predictions/'\n",
    "    WORK_DIR = '/kaggle/working/'\n",
    "\n",
    "DIRS = {\n",
    "    'models': os.path.join(WORK_DIR, 'models'),\n",
    "    'processed': os.path.join(WORK_DIR, 'processed_data'),\n",
    "    'encoders': os.path.join(WORK_DIR, 'encoders'),\n",
    "}\n",
    "\n",
    "for d in DIRS.values():\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T18:32:38.270687Z",
     "iopub.status.busy": "2025-12-17T18:32:38.269710Z",
     "iopub.status.idle": "2025-12-17T18:32:38.371932Z",
     "shell.execute_reply": "2025-12-17T18:32:38.370754Z",
     "shell.execute_reply.started": "2025-12-17T18:32:38.270636Z"
    }
   },
   "outputs": [],
   "source": [
    "# C-index Computation\n",
    "@jit(nopython=True)\n",
    "def init_btree(vals):\n",
    "    compare_times = np.empty_like(vals)\n",
    "    last_row_idx = int(np.log2(len(vals) + 1) - 1)\n",
    "    ragged_len = len(vals) - (2 ** (last_row_idx + 1) - 1)\n",
    "    \n",
    "    if ragged_len > 0:\n",
    "        btm_idx = slice(None, 2 * ragged_len, 2)\n",
    "        compare_times[-ragged_len:] = vals[btm_idx]\n",
    "        vals = np.delete(vals, btm_idx)\n",
    "    \n",
    "    start = 0\n",
    "    space = 2\n",
    "    length = 2 ** last_row_idx\n",
    "    \n",
    "    while start < len(vals):\n",
    "        compare_times[length - 1 : 2 * length - 1] = vals[start::space]\n",
    "        start += int(space / 2)\n",
    "        space *= 2\n",
    "        length = int(length / 2)\n",
    "    return compare_times\n",
    "\n",
    "@jit(nopython=True)\n",
    "def update_tree(counts, prediction, compare_times):\n",
    "    i = 0\n",
    "    n = len(compare_times)\n",
    "    while i < n:\n",
    "        current_val = compare_times[i]\n",
    "        counts[i] += 1\n",
    "        if prediction < current_val:\n",
    "            i = 2 * i + 1\n",
    "        elif prediction > current_val:\n",
    "            i = 2 * i + 2\n",
    "        else:\n",
    "            return counts\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_rank(prediction, compare_times, counts):\n",
    "    i = 0\n",
    "    n = len(compare_times)\n",
    "    rank = 0\n",
    "    count = 0\n",
    "    while i < n:\n",
    "        current_val = compare_times[i]\n",
    "        if prediction < current_val:\n",
    "            i = 2 * i + 1\n",
    "            continue\n",
    "        elif prediction > current_val:\n",
    "            rank += counts[i]\n",
    "            next_i = 2 * i + 2\n",
    "            if next_i < n:\n",
    "                rank -= counts[next_i]\n",
    "                i = next_i\n",
    "                continue\n",
    "            else:\n",
    "                return rank, count\n",
    "        else:\n",
    "            count = counts[i]\n",
    "            left_i = 2 * i + 1\n",
    "            if left_i < n:\n",
    "                left_count = counts[left_i]\n",
    "                count -= left_count\n",
    "                rank += left_count\n",
    "                right_i = left_i + 1\n",
    "                if right_i < n:\n",
    "                    count -= counts[right_i]\n",
    "            return rank, count\n",
    "    return rank, count\n",
    "\n",
    "@jit(nopython=True)\n",
    "def process_tied_pairs(truth, preds, start_idx, compare_times, counts):\n",
    "    next_idx = start_idx\n",
    "    while next_idx < len(truth) and truth[next_idx] == truth[start_idx]:\n",
    "        next_idx += 1\n",
    "        \n",
    "    pairs = counts[0] * (next_idx - start_idx)\n",
    "    correct = np.int64(0)\n",
    "    tied = np.int64(0)\n",
    "    \n",
    "    for i in range(start_idx, next_idx):\n",
    "        r, c = get_rank(preds[i], compare_times, counts)\n",
    "        correct += r\n",
    "        tied += c\n",
    "        \n",
    "    return (pairs, correct, tied, next_idx)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_cindex_calc(times, pred_times, event_observed):\n",
    "    died_mask = event_observed == 1\n",
    "    \n",
    "    died_truth = times[died_mask]\n",
    "    sort_idx = np.argsort(died_truth)\n",
    "    died_truth = died_truth[sort_idx]\n",
    "    died_preds = pred_times[died_mask][sort_idx]\n",
    "    \n",
    "    censored_truth = times[~died_mask]\n",
    "    sort_idx = np.argsort(censored_truth)\n",
    "    censored_truth = censored_truth[sort_idx]\n",
    "    censored_preds = pred_times[~died_mask][sort_idx]\n",
    "    \n",
    "    censored_ptr = 0\n",
    "    died_ptr = 0\n",
    "    \n",
    "    # Initialize tree\n",
    "    compare_times = init_btree(np.unique(died_preds))\n",
    "    counts = np.full(len(compare_times), 0)\n",
    "    \n",
    "    n_pairs = np.int64(0)\n",
    "    n_correct = np.int64(0)\n",
    "    n_tied = np.int64(0)\n",
    "    \n",
    "    while True:\n",
    "        has_censored = censored_ptr < len(censored_truth)\n",
    "        has_died = died_ptr < len(died_truth)\n",
    "        \n",
    "        if has_censored and (not has_died or died_truth[died_ptr] > censored_truth[censored_ptr]):\n",
    "            pairs, corr, tied, next_idx = process_tied_pairs(censored_truth, censored_preds, censored_ptr, compare_times, counts)\n",
    "            censored_ptr = next_idx\n",
    "            \n",
    "        elif has_died and (not has_censored or died_truth[died_ptr] <= censored_truth[censored_ptr]):\n",
    "            pairs, corr, tied, next_idx = process_tied_pairs(died_truth, died_preds, died_ptr, compare_times, counts)\n",
    "            \n",
    "            for p in died_preds[died_ptr:next_idx]:\n",
    "                update_tree(counts, p, compare_times)\n",
    "                \n",
    "            died_ptr = next_idx\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        n_pairs += pairs\n",
    "        n_correct += corr\n",
    "        n_tied += tied\n",
    "        \n",
    "    return (n_correct + n_tied / 2) / n_pairs\n",
    "\n",
    "def get_stratified_score(y, y_pred, events, race_groups):\n",
    "    df = pd.DataFrame({'y': y, 'y_pred': y_pred, 'events': events, 'race': race_groups})\n",
    "    scores = []\n",
    "    \n",
    "    for race_id, group_data in df.groupby('race'):\n",
    "        c_idx = fast_cindex_calc(\n",
    "            np.array(group_data['y']),\n",
    "            np.array(group_data['y_pred']),\n",
    "            np.array(group_data['events'])\n",
    "        )\n",
    "        scores.append(c_idx)\n",
    "        \n",
    "    mean_score = np.mean(scores)\n",
    "    penalty = np.sqrt(np.var(scores))\n",
    "    final_score = mean_score - penalty\n",
    "    return float(final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T18:33:01.610728Z",
     "iopub.status.busy": "2025-12-17T18:33:01.610373Z",
     "iopub.status.idle": "2025-12-17T18:33:03.341378Z",
     "shell.execute_reply": "2025-12-17T18:33:03.340128Z",
     "shell.execute_reply.started": "2025-12-17T18:33:01.610694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/equity-post-HCT-survival-predictions/train.csv...\n",
      "Processing /kaggle/input/equity-post-HCT-survival-predictions/test.csv...\n"
     ]
    }
   ],
   "source": [
    "def process_data(file_path, is_train=True):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    dict_df = pd.read_csv(os.path.join(BASE_DIR, 'data_dictionary.csv'))\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = list(dict_df[dict_df.type == 'Categorical']['variable'])\n",
    "    cat_cols = [c for c in cat_cols if c not in ['efs', 'efs_time']]\n",
    "    \n",
    "    num_cols = list(dict_df[dict_df.type == 'Numerical']['variable'])\n",
    "    num_cols = [c for c in num_cols if c not in ['efs', 'efs_time']]\n",
    "    \n",
    "    # Create string versions of some numerical columns (treating them as categorical)\n",
    "    for c in num_cols:\n",
    "        if c not in ['donor_age', 'age_at_hct']:\n",
    "            new_col = f\"{c}_str\"\n",
    "            df[new_col] = df[c].astype(str)\n",
    "            cat_cols.append(new_col)\n",
    "            \n",
    "    # Fill missing values\n",
    "    features_cat = df[cat_cols].fillna('-1').astype(str)\n",
    "    features_num = df[num_cols] # keep numerical as is\n",
    "    \n",
    "    # Combine them\n",
    "    X = pd.concat([features_cat, features_num], axis=1)\n",
    "    \n",
    "    # Handle One-Hot Encoding\n",
    "    if is_train:\n",
    "        # For training, we fit the encoder\n",
    "        oh_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        # We only OHE original categorical columns\n",
    "        orig_cat_cols = [c for c in list(dict_df[dict_df.type == 'Categorical']['variable']) if c not in ('efs', 'efs_time')]\n",
    "        \n",
    "        oh_encoded = oh_encoder.fit_transform(df[orig_cat_cols])\n",
    "        \n",
    "        # Save the encoder for later\n",
    "        with open(os.path.join(DIRS['encoders'], 'ohe.pkl'), 'wb') as f:\n",
    "            pickle.dump({'features': orig_cat_cols, 'model': oh_encoder}, f)\n",
    "    else:\n",
    "        # For test, load the encoder\n",
    "        with open(os.path.join(DIRS['encoders'], 'ohe.pkl'), 'rb') as f:\n",
    "            saved = pickle.load(f)\n",
    "        oh_encoder = saved['model']\n",
    "        orig_cat_cols = saved['features']\n",
    "        oh_encoded = oh_encoder.transform(df[orig_cat_cols])\n",
    "\n",
    "    # Convert OHE result to dataframe\n",
    "    oh_df = pd.DataFrame(oh_encoded, columns=[f'ohe_{i}' for i in range(oh_encoded.shape[1])])\n",
    "    X = pd.concat([X, oh_df], axis=1)\n",
    "    \n",
    "    # Identify feature lists\n",
    "    ohe_cols = list(oh_df.columns)\n",
    "    \n",
    "    result = {\n",
    "        'X': X,\n",
    "        'cat_cols': cat_cols,\n",
    "        'num_cols': num_cols,\n",
    "        'ohe_cols': ohe_cols,\n",
    "        'ID': df['ID'],\n",
    "        'race_group': df['race_group']\n",
    "    }\n",
    "    \n",
    "    if is_train:\n",
    "        result['efs'] = df['efs']\n",
    "        result['efs_time'] = df['efs_time']\n",
    "        \n",
    "    return result\n",
    "\n",
    "# --- Run the processing ---\n",
    "train_data = process_data(os.path.join(BASE_DIR, 'train.csv'), is_train=True)\n",
    "with open(os.path.join(DIRS['processed'], 'train_packed.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "test_data = process_data(os.path.join(BASE_DIR, 'test.csv'), is_train=False)\n",
    "with open(os.path.join(DIRS['processed'], 'test_packed.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T18:33:14.485425Z",
     "iopub.status.busy": "2025-12-17T18:33:14.485082Z",
     "iopub.status.idle": "2025-12-17T18:54:17.215308Z",
     "shell.execute_reply": "2025-12-17T18:54:17.214175Z",
     "shell.execute_reply.started": "2025-12-17T18:33:14.485399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost Classifier...\n",
      "--- Fold 0 ---\n",
      "0:\ttest: 0.6702781\tbest: 0.6702781 (0)\ttotal: 212ms\tremaining: 7m 2s\n",
      "500:\ttest: 0.7565740\tbest: 0.7565740 (500)\ttotal: 1m 2s\tremaining: 3m 7s\n",
      "1000:\ttest: 0.7610635\tbest: 0.7610635 (1000)\ttotal: 2m 5s\tremaining: 2m 4s\n",
      "1500:\ttest: 0.7620212\tbest: 0.7621786 (1352)\ttotal: 3m 7s\tremaining: 1m 2s\n",
      "1999:\ttest: 0.7621559\tbest: 0.7622603 (1666)\ttotal: 4m 11s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7622603455\n",
      "bestIteration = 1666\n",
      "\n",
      "--- Fold 1 ---\n",
      "0:\ttest: 0.6873330\tbest: 0.6873330 (0)\ttotal: 159ms\tremaining: 5m 18s\n",
      "500:\ttest: 0.7539659\tbest: 0.7539659 (500)\ttotal: 1m 2s\tremaining: 3m 7s\n",
      "1000:\ttest: 0.7580478\tbest: 0.7580677 (999)\ttotal: 2m 6s\tremaining: 2m 5s\n",
      "1500:\ttest: 0.7591167\tbest: 0.7591718 (1489)\ttotal: 3m 10s\tremaining: 1m 3s\n",
      "1999:\ttest: 0.7597379\tbest: 0.7598889 (1969)\ttotal: 4m 13s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7598888507\n",
      "bestIteration = 1969\n",
      "\n",
      "--- Fold 2 ---\n",
      "0:\ttest: 0.6697805\tbest: 0.6697805 (0)\ttotal: 138ms\tremaining: 4m 35s\n",
      "500:\ttest: 0.7570486\tbest: 0.7570486 (500)\ttotal: 1m 2s\tremaining: 3m 8s\n",
      "1000:\ttest: 0.7606891\tbest: 0.7607442 (979)\ttotal: 2m 5s\tremaining: 2m 5s\n",
      "1500:\ttest: 0.7614363\tbest: 0.7615366 (1329)\ttotal: 3m 9s\tremaining: 1m 2s\n",
      "1999:\ttest: 0.7611581\tbest: 0.7616295 (1582)\ttotal: 4m 12s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.761629532\n",
      "bestIteration = 1582\n",
      "\n",
      "--- Fold 3 ---\n",
      "0:\ttest: 0.6690374\tbest: 0.6690374 (0)\ttotal: 144ms\tremaining: 4m 48s\n",
      "500:\ttest: 0.7549690\tbest: 0.7549918 (498)\ttotal: 1m 2s\tremaining: 3m 6s\n",
      "1000:\ttest: 0.7596601\tbest: 0.7596601 (1000)\ttotal: 2m 4s\tremaining: 2m 4s\n",
      "1500:\ttest: 0.7614340\tbest: 0.7614815 (1496)\ttotal: 3m 7s\tremaining: 1m 2s\n",
      "1999:\ttest: 0.7616068\tbest: 0.7616777 (1816)\ttotal: 4m 11s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.761677697\n",
      "bestIteration = 1816\n",
      "\n",
      "--- Fold 4 ---\n",
      "0:\ttest: 0.6894566\tbest: 0.6894566 (0)\ttotal: 143ms\tremaining: 4m 45s\n",
      "500:\ttest: 0.7539807\tbest: 0.7539807 (500)\ttotal: 1m 1s\tremaining: 3m 4s\n",
      "1000:\ttest: 0.7590843\tbest: 0.7590856 (998)\ttotal: 2m 3s\tremaining: 2m 3s\n",
      "1500:\ttest: 0.7602104\tbest: 0.7602480 (1494)\ttotal: 3m 6s\tremaining: 1m 1s\n",
      "1999:\ttest: 0.7607700\tbest: 0.7609789 (1896)\ttotal: 4m 9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7609789114\n",
      "bestIteration = 1896\n",
      "\n",
      "Overall OOF AUC: 0.7610221258516461\n"
     ]
    }
   ],
   "source": [
    "def prep_for_catboost(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    for c in data['cat_cols']:\n",
    "        data['X'][c] = data['X'][c].astype('category')\n",
    "        \n",
    "    # Filter only columns we want\n",
    "    use_cols = data['cat_cols'] + data['num_cols'] + data['ohe_cols']\n",
    "    data['X'] = data['X'][use_cols]\n",
    "    \n",
    "    # Save category info to ensure consistency\n",
    "    cats = {c: data['X'][c].cat.categories for c in data['X'].columns if data['X'][c].dtype == 'category'}\n",
    "    return data, cats\n",
    "\n",
    "def train_classifier(folds=5, seed=43):\n",
    "    print(\"Training CatBoost Classifier...\")\n",
    "    data, cat_map = prep_for_catboost(os.path.join(DIRS['processed'], 'train_packed.pkl'))\n",
    "    \n",
    "    with open(os.path.join(DIRS['encoders'], 'cat_map_cls.pkl'), 'wb') as f:\n",
    "        pickle.dump(cat_map, f)\n",
    "        \n",
    "    # Target: 1 if Event DID NOT happen (censored), 0 if it DID\n",
    "    targets = (data['efs'] == 0).astype(int)\n",
    "    \n",
    "    params = {\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'cat_features': data['cat_cols'],\n",
    "        'verbose': 500,\n",
    "        'random_seed': seed,\n",
    "        'depth': 6,\n",
    "        'learning_rate': 0.02,\n",
    "        'n_estimators': 2000,\n",
    "        'l2_leaf_reg': 1,\n",
    "        'subsample': 0.66,\n",
    "        'colsample_bylevel': 0.8\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Stratified K-Fold based on race and event status\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=888)\n",
    "    split_target = data['efs'].astype(str) + '_' + data['race_group'].astype(str)\n",
    "    \n",
    "    oof_preds = np.zeros(len(data['X']))\n",
    "    \n",
    "    for fold_idx, (idx_tr, idx_val) in enumerate(skf.split(data['X'], split_target)):\n",
    "        print(f\"--- Fold {fold_idx} ---\")\n",
    "        X_tr, y_tr = data['X'].iloc[idx_tr], targets.iloc[idx_tr]\n",
    "        X_val, y_val = data['X'].iloc[idx_val], targets.iloc[idx_val]\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            use_best_model=False\n",
    "        )\n",
    "        \n",
    "        # Predict probability\n",
    "        val_preds = model.predict_proba(X_val)[:, 1]\n",
    "        oof_preds[idx_val] = val_preds\n",
    "        \n",
    "        # Save this fold's model\n",
    "        payload = {'tr_idx': idx_tr, 'val_idx': idx_val, 'model': model}\n",
    "        with open(os.path.join(DIRS['models'], f'catboost_cls_f{fold_idx}.pkl'), 'wb') as f:\n",
    "            pickle.dump(payload, f)\n",
    "            \n",
    "    print(f\"Overall OOF AUC: {roc_auc_score(targets, oof_preds)}\")\n",
    "\n",
    "# Run it\n",
    "train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T18:56:49.352796Z",
     "iopub.status.busy": "2025-12-17T18:56:49.352374Z",
     "iopub.status.idle": "2025-12-17T19:01:38.229191Z",
     "shell.execute_reply": "2025-12-17T19:01:38.228081Z",
     "shell.execute_reply.started": "2025-12-17T18:56:49.352759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM Regressor...\n",
      "--- Fold 0 ---\n",
      "--- Fold 1 ---\n",
      "--- Fold 2 ---\n",
      "--- Fold 3 ---\n",
      "--- Fold 4 ---\n",
      "OOF C-Index (Events only): 0.7637697447664714\n"
     ]
    }
   ],
   "source": [
    "def prep_for_lgb(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    use_cols = data['cat_cols'] + data['num_cols'] + data['ohe_cols']\n",
    "    # Removing some specific features deemed noisy\n",
    "    drops = ['year_hct_str', 'hla_match_a_high_str', 'hla_match_b_low_str', 'comorbidity_score_str']\n",
    "    final_cols = [c for c in use_cols if f\"{c}_str\" not in drops and c not in drops] \n",
    "    \n",
    "    X = data['X'].copy()\n",
    "    drop_actual = []\n",
    "    for d in drops:\n",
    "        if d in X.columns: drop_actual.append(d)\n",
    "    X = X.drop(drop_actual, axis=1)\n",
    "    \n",
    "    # Convert to category for LGBM\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == 'object':\n",
    "            X[c] = X[c].astype('category')\n",
    "            \n",
    "    cats = {c: X[c].cat.categories for c in X.columns if X[c].dtype.name == 'category'}\n",
    "    data['X'] = X\n",
    "    return data, cats\n",
    "\n",
    "def train_regressor(folds=5, seed=42):\n",
    "    print(\"Training LightGBM Regressor...\")\n",
    "    data, cat_map = prep_for_lgb(os.path.join(DIRS['processed'], 'train_packed.pkl'))\n",
    "    \n",
    "    with open(os.path.join(DIRS['encoders'], 'cat_map_lgb.pkl'), 'wb') as f:\n",
    "        pickle.dump(cat_map, f)\n",
    "        \n",
    "    # --- Custom Target Engineering ---\n",
    "    # We rank the times separately for people who had the event vs those who didn't.\n",
    "    norm_time = data['efs_time'].copy()\n",
    "    mask_event = data['efs'] == 1\n",
    "    mask_sensor = data['efs'] == 0\n",
    "    \n",
    "    norm_time[mask_event] = data['efs_time'][mask_event].rank() / mask_event.sum()\n",
    "    norm_time[mask_sensor] = data['efs_time'][mask_sensor].rank() / mask_sensor.sum()\n",
    "    \n",
    "    # Weights: give more importance to actual events (0.6) vs censored (0.4)\n",
    "    weights = np.where(mask_event, 0.6, 0.4)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'learning_rate': 0.02,\n",
    "        'num_iterations': 10000,\n",
    "        'num_leaves': 128,\n",
    "        'max_depth': 6,\n",
    "        'min_child_samples': 20,\n",
    "        'extra_trees': True,\n",
    "        'max_bin': 128,\n",
    "        'verbose': -1,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=888)\n",
    "    split_target = data['efs'].astype(str) + '_' + data['race_group'].astype(str)\n",
    "    \n",
    "    oof_preds = np.zeros(len(data['X']))\n",
    "    \n",
    "    for fold_idx, (idx_tr, idx_val) in enumerate(skf.split(data['X'], split_target)):\n",
    "        print(f\"--- Fold {fold_idx} ---\")\n",
    "        X_tr, y_tr = data['X'].iloc[idx_tr].copy(), norm_time.iloc[idx_tr]\n",
    "        X_val, y_val = data['X'].iloc[idx_val].copy(), norm_time.iloc[idx_val]\n",
    "        w_tr = weights[idx_tr]\n",
    "        \n",
    "        # Add 'efs' as a feature for training (cheat code for training, fixed for inference)\n",
    "        X_tr['efs'] = data['efs'].iloc[idx_tr].astype('int').astype('category')\n",
    "        X_val['efs'] = data['efs'].iloc[idx_val].astype('int').astype('category')\n",
    "        \n",
    "        # Focus evaluation only on events that actually happened\n",
    "        mask_eval_event = data['efs'].iloc[idx_val] == 1\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            sample_weight=w_tr,\n",
    "            eval_set=[(X_val[mask_eval_event], y_val[mask_eval_event])],\n",
    "            callbacks=[]\n",
    "        )\n",
    "        \n",
    "        oof_preds[idx_val] = model.predict(X_val)\n",
    "        \n",
    "        payload = {'tr_idx': idx_tr, 'val_idx': idx_val, 'model': model}\n",
    "        with open(os.path.join(DIRS['models'], f'lgb_reg_f{fold_idx}.pkl'), 'wb') as f:\n",
    "            pickle.dump(payload, f)\n",
    "            \n",
    "    c_idx = concordance_index(data['efs_time'][mask_event], oof_preds[mask_event], data['efs'][mask_event])\n",
    "    print(f\"OOF C-Index (Events only): {c_idx}\")\n",
    "\n",
    "train_regressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T19:01:38.230668Z",
     "iopub.status.busy": "2025-12-17T19:01:38.230380Z",
     "iopub.status.idle": "2025-12-17T19:02:49.203380Z",
     "shell.execute_reply": "2025-12-17T19:02:49.202260Z",
     "shell.execute_reply.started": "2025-12-17T19:01:38.230643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing blend for Fold 0...\n",
      "Best: {'a': 2.7922221319022764, 'b': 0.7698711256179719, 'c': 0.9127555081932986}\n",
      "Optimizing blend for Fold 1...\n",
      "Best: {'a': 2.737268527282331, 'b': 1.1942366971205687, 'c': 0.17762761044190378}\n",
      "Optimizing blend for Fold 2...\n",
      "Best: {'a': 3.374492437083786, 'b': 1.0165893153511152, 'c': 0.16606093106065467}\n",
      "Optimizing blend for Fold 3...\n",
      "Best: {'a': 2.327916207954, 'b': 1.1242979623333207, 'c': 0.43744448919508716}\n",
      "Optimizing blend for Fold 4...\n",
      "Best: {'a': 3.1334868282104167, 'b': 0.6146091190702917, 'c': 0.13841383528214324}\n"
     ]
    }
   ],
   "source": [
    "def get_cls_preds(is_test=False, folds=5):\n",
    "    # Load data\n",
    "    fname = 'test_packed.pkl' if is_test else 'train_packed.pkl'\n",
    "    data, _ = prep_for_catboost(os.path.join(DIRS['processed'], fname))\n",
    "    \n",
    "    # Restore categories\n",
    "    with open(os.path.join(DIRS['encoders'], 'cat_map_cls.pkl'), 'rb') as f:\n",
    "        cat_map = pickle.load(f)\n",
    "    for c, cats in cat_map.items():\n",
    "        if c in data['X'].columns:\n",
    "            data['X'][c] = data['X'][c].astype('category').cat.set_categories(cats)\n",
    "            \n",
    "    preds = np.zeros(len(data['X']))\n",
    "    \n",
    "    for i in range(folds):\n",
    "        with open(os.path.join(DIRS['models'], f'catboost_cls_f{i}.pkl'), 'rb') as f:\n",
    "            pkg = pickle.load(f)\n",
    "        \n",
    "        model = pkg['model']\n",
    "        if is_test:\n",
    "            preds += model.predict_proba(data['X'])[:, 1] / folds\n",
    "        else:\n",
    "            # For OOF, only predict on validation set\n",
    "            val_idx = pkg['val_idx']\n",
    "            preds[val_idx] = model.predict_proba(data['X'].iloc[val_idx])[:, 1]\n",
    "            \n",
    "    return preds\n",
    "\n",
    "def get_reg_preds(is_test=False, folds=5):\n",
    "    fname = 'test_packed.pkl' if is_test else 'train_packed.pkl'\n",
    "    data, _ = prep_for_lgb(os.path.join(DIRS['processed'], fname))\n",
    "    \n",
    "    with open(os.path.join(DIRS['encoders'], 'cat_map_lgb.pkl'), 'rb') as f:\n",
    "        cat_map = pickle.load(f)\n",
    "    for c, cats in cat_map.items():\n",
    "        if c in data['X'].columns:\n",
    "            data['X'][c] = data['X'][c].astype('category').cat.set_categories(cats)\n",
    "            \n",
    "    # CRITICAL: Set 'efs' to 1 for inference\n",
    "    data['X']['efs'] = 1\n",
    "    data['X']['efs'] = data['X']['efs'].astype('category')\n",
    "    \n",
    "    preds = np.zeros(len(data['X']))\n",
    "    \n",
    "    for i in range(folds):\n",
    "        with open(os.path.join(DIRS['models'], f'lgb_reg_f{i}.pkl'), 'rb') as f:\n",
    "            pkg = pickle.load(f)\n",
    "        model = pkg['model']\n",
    "        \n",
    "        if is_test:\n",
    "            preds += model.predict(data['X']) / folds\n",
    "        else:\n",
    "            val_idx = pkg['val_idx']\n",
    "            preds[val_idx] = model.predict(data['X'].iloc[val_idx])\n",
    "            \n",
    "    return preds\n",
    "\n",
    "def blend_preds(reg_p, cls_p, a, b, c):\n",
    "    # reg_p = time prediction, cls_p = probability prediction\n",
    "    term_y = (reg_p > 0) * c * np.abs(reg_p)**b\n",
    "    term_x = (cls_p > 0) * np.abs(cls_p)**a\n",
    "    \n",
    "    # Merge Function\n",
    "    combined = (1 - term_y) * term_x + term_y\n",
    "    \n",
    "    # Return ranks\n",
    "    return pd.Series(combined).rank() / len(combined)\n",
    "\n",
    "def optimize_blending():\n",
    "    with open(os.path.join(DIRS['processed'], 'train_packed.pkl'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    efs, times, race = data['efs'], data['efs_time'], data['race_group']\n",
    "    \n",
    "    # Get predictions\n",
    "    p_cls = get_cls_preds(is_test=False)\n",
    "    p_reg = get_reg_preds(is_test=False)\n",
    "    \n",
    "    # Optuna Objective\n",
    "    def objective(trial, idx):\n",
    "        a = trial.suggest_uniform('a', 2, 3.5)\n",
    "        b = trial.suggest_uniform('b', 0.5, 1.5)\n",
    "        c = trial.suggest_uniform('c', 0, 1)\n",
    "        \n",
    "        final_p = blend_preds(p_reg[idx], p_cls[idx], a, b, c)\n",
    "        score = get_stratified_score(times[idx], final_p, efs[idx], race[idx])\n",
    "        return score\n",
    "\n",
    "    # Optimize per fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=888)\n",
    "    split_target = efs.astype(str) + '_' + race.astype(str)\n",
    "    \n",
    "    best_params_list = []\n",
    "    \n",
    "    for i, (tr_idx, val_idx) in enumerate(skf.split(efs, split_target)):\n",
    "        print(f\"Optimizing blend for Fold {i}...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda t: objective(t, val_idx), n_trials=200)\n",
    "        best_params_list.append(study.best_params)\n",
    "        print(f\"Best: {study.best_params}\")\n",
    "        \n",
    "    return best_params_list\n",
    "\n",
    "# Find best parameters\n",
    "blend_params = optimize_blending()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T19:02:49.205401Z",
     "iopub.status.busy": "2025-12-17T19:02:49.204927Z",
     "iopub.status.idle": "2025-12-17T19:02:51.313207Z",
     "shell.execute_reply": "2025-12-17T19:02:51.311782Z",
     "shell.execute_reply.started": "2025-12-17T19:02:49.205365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Final Submission...\n",
      "Done! Submission saved.\n",
      "      ID  prediction\n",
      "0  28800    0.333333\n",
      "1  28801    0.666667\n",
      "2  28802    0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Final Submission...\")\n",
    "\n",
    "test_cls = get_cls_preds(is_test=True)\n",
    "test_reg = get_reg_preds(is_test=True)\n",
    "\n",
    "final_blend = []\n",
    "for params in blend_params:\n",
    "    res = blend_preds(test_reg, test_cls, **params)\n",
    "    final_blend.append(res)\n",
    "\n",
    "avg_preds = np.mean(np.array(final_blend).T, axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'prediction': 1 - avg_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Done! Submission saved.\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
