{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b4b6c8",
   "metadata": {
    "papermill": {
     "duration": 0.004825,
     "end_time": "2025-03-12T02:52:30.455986",
     "exception": false,
     "start_time": "2025-03-12T02:52:30.451161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Decripition\n",
    "- this is the simplified version only with catboost classifer and lightgbm regressor\n",
    "- https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/566550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a1c3c0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-12T02:52:30.466219Z",
     "iopub.status.busy": "2025-03-12T02:52:30.465858Z",
     "iopub.status.idle": "2025-03-12T02:52:56.766440Z",
     "shell.execute_reply": "2025-03-12T02:52:56.764952Z"
    },
    "papermill": {
     "duration": 26.308088,
     "end_time": "2025-03-12T02:52:56.768767",
     "exception": false,
     "start_time": "2025-03-12T02:52:30.460679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd==1.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->autograd==1.7.0) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd==1.7.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd==1.7.0) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->autograd==1.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->autograd==1.7.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->autograd==1.7.0) (2024.2.0)\r\n",
      "autograd is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\r\n",
      "Requirement already satisfied: autograd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.7.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from autograd-gamma==0.5.0) (1.13.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autograd>=1.2.0->autograd-gamma==0.5.0) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->autograd>=1.2.0->autograd-gamma==0.5.0) (2024.2.0)\r\n",
      "Building wheels for collected packages: autograd-gamma\r\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001B[?25l\u001B[?25hdone\r\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=9f8076ba5680562da4d8214dd62e129e0f2f91c433fd012de4dc85bfeddc24c3\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/b5/e0/4c79e15c0b5f2c15ecf613c720bb20daab20a666eb67135155\r\n",
      "Successfully built autograd-gamma\r\n",
      "Installing collected packages: autograd-gamma\r\n",
      "Successfully installed autograd-gamma-0.5.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\r\n",
      "Installing collected packages: interface-meta\r\n",
      "Successfully installed interface-meta-1.3.0\r\n",
      "Processing /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (2.2.2)\r\n",
      "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic==1.0.2) (1.17.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.5->formulaic==1.0.2) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->formulaic==1.0.2) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic==1.0.2) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.5->formulaic==1.0.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.5->formulaic==1.0.2) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.5->formulaic==1.0.2) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.5->formulaic==1.0.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.5->formulaic==1.0.2) (2024.2.0)\r\n",
      "Installing collected packages: formulaic\r\n",
      "Successfully installed formulaic-1.0.2\r\n",
      "Processing /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.13.1)\r\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (2.2.2)\r\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (3.7.5)\r\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.7.0)\r\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (0.5.0)\r\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines==0.30.0) (1.0.2)\r\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines==0.30.0) (1.17.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (24.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines==0.30.0) (2.8.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->lifelines==0.30.0) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines==0.30.0) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.30.0) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->lifelines==0.30.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->lifelines==0.30.0) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->lifelines==0.30.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->lifelines==0.30.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->lifelines==0.30.0) (2024.2.0)\r\n",
      "Installing collected packages: lifelines\r\n",
      "Successfully installed lifelines-0.30.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ea03ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T02:52:56.783287Z",
     "iopub.status.busy": "2025-03-12T02:52:56.782877Z",
     "iopub.status.idle": "2025-03-12T02:53:05.070090Z",
     "shell.execute_reply": "2025-03-12T02:53:05.068829Z"
    },
    "papermill": {
     "duration": 8.296962,
     "end_time": "2025-03-12T02:53:05.072405",
     "exception": false,
     "start_time": "2025-03-12T02:52:56.775443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from lifelines.utils import concordance_index\n",
    "from matplotlib import pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "from sklearn.metrics import roc_auc_score,mean_absolute_error\n",
    "import os\n",
    "from lifelines.utils import concordance_index\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score,mean_absolute_error\n",
    "import itertools\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from numba import jit\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9621bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T02:53:05.085953Z",
     "iopub.status.busy": "2025-03-12T02:53:05.085197Z",
     "iopub.status.idle": "2025-03-12T02:53:05.174018Z",
     "shell.execute_reply": "2025-03-12T02:53:05.172774Z"
    },
    "papermill": {
     "duration": 0.097809,
     "end_time": "2025-03-12T02:53:05.176128",
     "exception": false,
     "start_time": "2025-03-12T02:53:05.078319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def init_BTree(values):\n",
    "    times_to_compare = np.empty_like(values)\n",
    "    last_full_row = int(np.log2(len(values) + 1) - 1)\n",
    "    len_ragged_row = len(values) - (2 ** (last_full_row + 1) - 1)\n",
    "    if len_ragged_row > 0:\n",
    "#        bottom_row_ix = np.s_[: 2 * len_ragged_row : 2]\n",
    "        bottom_row_ix = slice(None, 2 * len_ragged_row, 2)\n",
    "        times_to_compare[-len_ragged_row:] = values[bottom_row_ix]\n",
    "        values = np.delete(values, bottom_row_ix)\n",
    "    values_start = 0\n",
    "    values_space = 2\n",
    "    values_len = 2 ** last_full_row\n",
    "    while values_start < len(values):\n",
    "        times_to_compare[values_len - 1 : 2 * values_len - 1] = values[values_start::values_space]\n",
    "        values_start += int(values_space / 2)\n",
    "        values_space *= 2\n",
    "        values_len = int(values_len / 2)\n",
    "    return times_to_compare\n",
    "\n",
    "@jit(nopython=True)\n",
    "def insert(counts, pred, times_to_compare):\n",
    "    i = 0\n",
    "    n = len(times_to_compare)\n",
    "    while (i < n):\n",
    "        cur = times_to_compare[i]\n",
    "        counts[i] += 1\n",
    "        if pred < cur:\n",
    "            i = 2 * i + 1\n",
    "        elif pred > cur:\n",
    "            i = 2 * i + 2\n",
    "        else:\n",
    "            return counts\n",
    "    #raise ValueError(\"Value %s not contained in tree.\" \"Also, the counts are now messed up.\" % times_to_compare)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fn_rank(pred, times_to_compare, counts):\n",
    "    i = 0\n",
    "    n = len(times_to_compare)\n",
    "    rank = 0\n",
    "    count = 0\n",
    "    while (i < n):\n",
    "        cur = times_to_compare[i]\n",
    "        if pred < cur:\n",
    "            i = 2 * i + 1\n",
    "            continue\n",
    "        elif pred > cur:\n",
    "            rank += counts[i]\n",
    "            # subtract off the right tree if exists\n",
    "            nexti = 2 * i + 2\n",
    "            if nexti < n:\n",
    "                rank -= counts[nexti]\n",
    "                i = nexti\n",
    "                continue\n",
    "            else:\n",
    "                return rank, count\n",
    "        else:  # value == cur\n",
    "            count = counts[i]\n",
    "            lefti = 2 * i + 1\n",
    "            if lefti < n:\n",
    "                nleft = counts[lefti]\n",
    "                count -= nleft\n",
    "                rank += nleft\n",
    "                righti = lefti + 1\n",
    "                if righti < n:\n",
    "                    count -= counts[righti]\n",
    "            return rank, count\n",
    "    return rank, count\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def handle_pairs(truth, pred, first_ix, times_to_compare, counts):\n",
    "    next_ix = first_ix\n",
    "    while next_ix < len(truth) and truth[next_ix] == truth[first_ix]:\n",
    "        next_ix += 1\n",
    "    pairs = counts[0] * (next_ix - first_ix)\n",
    "    correct = np.int64(0)\n",
    "    tied = np.int64(0)\n",
    "    for i in range(first_ix, next_ix):\n",
    "#        rank, count = times_to_compare.rank(censored_pred[i])\n",
    "        rank, count = fn_rank(pred[i], times_to_compare, counts)\n",
    "        correct += rank\n",
    "        tied += count\n",
    "    return (pairs, correct, tied, next_ix)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_concordance_index(event_times, predicted_event_times, event_observed):\n",
    "    \n",
    "    died_mask = event_observed==1#.astype(bool)\n",
    "    # TODO: is event_times already sorted? That would be nice...\n",
    "    died_truth = event_times[died_mask]\n",
    "    ix = np.argsort(died_truth)\n",
    "    died_truth = died_truth[ix]\n",
    "    died_pred = predicted_event_times[died_mask][ix]\n",
    "\n",
    "    censored_truth = event_times[~died_mask]\n",
    "    ix = np.argsort(censored_truth)\n",
    "    censored_truth = censored_truth[ix]\n",
    "    censored_pred = predicted_event_times[~died_mask][ix]\n",
    "\n",
    "    censored_ix = 0\n",
    "    died_ix = 0\n",
    "    \n",
    "    times_to_compare = init_BTree(np.unique(died_pred))\n",
    "#    counts = np.zeros_like(times_to_compare, dtype=int)\n",
    "    counts = np.full(len(times_to_compare), 0)\n",
    "    \n",
    "    num_pairs = np.int64(0)\n",
    "    num_correct = np.int64(0)\n",
    "    num_tied = np.int64(0)\n",
    "\n",
    "    # we iterate through cases sorted by exit time:\n",
    "    # - First, all cases that died at time t0. We add these to the sortedlist of died times.\n",
    "    # - Then, all cases that were censored at time t0. We DON'T add these since they are NOT\n",
    "    #   comparable to subsequent elements.\n",
    "    while True:\n",
    "        has_more_censored = censored_ix < len(censored_truth)\n",
    "        has_more_died = died_ix < len(died_truth)\n",
    "        # Should we look at some censored indices next, or died indices?\n",
    "        if has_more_censored and (not has_more_died or died_truth[died_ix] > censored_truth[censored_ix]):\n",
    "            pairs, correct, tied, next_ix = handle_pairs(censored_truth, censored_pred, censored_ix, times_to_compare, counts)\n",
    "            censored_ix = next_ix\n",
    "            \n",
    "        elif has_more_died and (not has_more_censored or died_truth[died_ix] <= censored_truth[censored_ix]):\n",
    "            pairs, correct, tied, next_ix = handle_pairs(died_truth, died_pred, died_ix, times_to_compare, counts)\n",
    "\n",
    "            for pred in died_pred[died_ix:next_ix]:\n",
    "                insert(counts, pred, times_to_compare)\n",
    "                                \n",
    "            died_ix = next_ix\n",
    "        else:\n",
    "            assert not (has_more_died or has_more_censored)\n",
    "            break\n",
    "\n",
    "        num_pairs += pairs\n",
    "        num_correct += correct\n",
    "        num_tied += tied\n",
    "        \n",
    "#    print(num_pairs, num_correct, num_tied)\n",
    "\n",
    "    return (num_correct + num_tied / 2) / num_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51186ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T02:53:05.189343Z",
     "iopub.status.busy": "2025-03-12T02:53:05.188976Z",
     "iopub.status.idle": "2025-03-12T02:53:05.198104Z",
     "shell.execute_reply": "2025-03-12T02:53:05.197055Z"
    },
    "papermill": {
     "duration": 0.01765,
     "end_time": "2025-03-12T02:53:05.200000",
     "exception": false,
     "start_time": "2025-03-12T02:53:05.182350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "local=False\n",
    "\n",
    "class global_args():\n",
    "    def __init__(self,local):\n",
    "        if local:\n",
    "            self.data_dir='./data/'\n",
    "            self.model_dir='./models/'\n",
    "            self.data_process_dir='./data_process/'\n",
    "            self.submission_dir='./submission/'\n",
    "            self.encoder_info_dir='./encoder_info/'\n",
    "            self.fold_n=5\n",
    "        else:\n",
    "            self.data_dir='/kaggle/input/equity-post-HCT-survival-predictions/'\n",
    "            self.model_dir='/kaggle/working/models/'\n",
    "            self.data_process_dir='/kaggle/working/data_process/'\n",
    "            self.submission_dir='/kaggle/working/'\n",
    "            self.encoder_info_dir='/kaggle/working/encoder_info/'\n",
    "            self.fold_n=5\n",
    "\n",
    "\n",
    "args = global_args(local=local)\n",
    "\n",
    "if local==False:\n",
    "    os.mkdir(args.model_dir)\n",
    "    os.mkdir(args.data_process_dir)\n",
    "    os.mkdir(args.encoder_info_dir)\n",
    "\n",
    "\n",
    "def CIBMTR_score(y, y_hat,efs,race_group):\n",
    "    merged_df = pd.DataFrame({'y':y,'y_hat':y_hat,'efs':efs,'race_group':race_group})\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    race_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        c_index_race = fast_concordance_index(\n",
    "                        np.array(merged_df_race['y']),\n",
    "                        np.array(merged_df_race['y_hat']),\n",
    "                        np.array(merged_df_race['efs']))\n",
    "        metric_list.append(c_index_race)\n",
    "        race_list.append(race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list))),np.sqrt(np.var(metric_list)),{race:cindex for race,cindex in zip(race_list,metric_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f62183",
   "metadata": {
    "papermill": {
     "duration": 0.005294,
     "end_time": "2025-03-12T02:53:05.210976",
     "exception": false,
     "start_time": "2025-03-12T02:53:05.205682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DataProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71ce00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T02:53:05.224189Z",
     "iopub.status.busy": "2025-03-12T02:53:05.223617Z",
     "iopub.status.idle": "2025-03-12T02:53:07.443760Z",
     "shell.execute_reply": "2025-03-12T02:53:07.442592Z"
    },
    "papermill": {
     "duration": 2.228568,
     "end_time": "2025-03-12T02:53:07.445630",
     "exception": false,
     "start_time": "2025-03-12T02:53:05.217062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categary Feature number: 55\n",
      "numerical Feature number: 22\n",
      "One-hot Feature number: 191\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(data_file,train):\n",
    "    data = pd.read_csv(data_file)\n",
    "    data_dict = pd.read_csv(args.data_dir+'data_dictionary.csv')\n",
    "    feature_cat = list(data_dict.loc[data_dict.type=='Categorical','variable'])\n",
    "    feature_cat = [x for x in feature_cat if x  not in ('efs','efs_time')]\n",
    "    feature_value = list(data_dict.loc[data_dict.type=='Numerical','variable'])    \n",
    "    feature_value = [x for x in feature_value if x  not in ('efs','efs_time')]\n",
    "\n",
    "    for col in feature_value:\n",
    "        if col not in ['donor_age','age_at_hct']:\n",
    "            data[col+'_trans2cat'] = data[col].copy().astype('str')\n",
    "            feature_cat.append(col+'_trans2cat')\n",
    "    \n",
    "\n",
    "    features = data[feature_cat].fillna('-1')\n",
    "    for col in features.columns:\n",
    "        features[col]=features[col].astype('str')\n",
    "\n",
    "    features_new = data[feature_value]#.fillna(-1)\n",
    "    features = pd.concat([features,features_new],axis=1)\n",
    "    \n",
    "\n",
    "    with open(args.encoder_info_dir+'one_hot_encoder.pkl','rb') as f:\n",
    "        one_hot_encoder = pickle.load(f)\n",
    "    \n",
    "    # one-hot\n",
    "    tmp = one_hot_encoder['features']\n",
    "    features_new = one_hot_encoder['model'].transform(data[tmp]) \n",
    "    features_new=pd.DataFrame(features_new)\n",
    "    features_new.columns = ['onehot_'+str(x) for x in range(len(features_new.columns))]\n",
    "    features = pd.concat([features,features_new],axis=1)\n",
    "    feature_onehot = list(features_new.columns)\n",
    "    \n",
    "\n",
    "    if train==True:\n",
    "        return {'X':features,\n",
    "                'efs':data.efs,\n",
    "                'efs_time':data.efs_time,\n",
    "                'feature_cat':feature_cat,\n",
    "                'feature_value':feature_value,\n",
    "                'feature_onehot':feature_onehot,\n",
    "                'ID':data.ID,\n",
    "                'race_group':data.race_group}\n",
    "    else:\n",
    "        return {'X':features,\n",
    "                'feature_cat':feature_cat,\n",
    "                'feature_value':feature_value,\n",
    "                'feature_onehot':feature_onehot,\n",
    "                'ID':data.ID,\n",
    "                'race_group':data.race_group}\n",
    "\n",
    "\n",
    "data = pd.read_csv(args.data_dir+'train.csv')\n",
    "data_dict = pd.read_csv(args.data_dir+'data_dictionary.csv')\n",
    "\n",
    "\n",
    "# one-hot encoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_features = list(data_dict.loc[data_dict.type=='Categorical','variable'])\n",
    "one_hot_features = [x for x in one_hot_features if x not in ('efs','efs_time')]\n",
    "one_hot_encoder.fit(data[one_hot_features])\n",
    "one_hot_encoder_info  ={'features':one_hot_features,'model':one_hot_encoder}\n",
    "with open(args.encoder_info_dir+'one_hot_encoder.pkl','wb') as f:\n",
    "    pickle.dump(one_hot_encoder_info, f)\n",
    "\n",
    "# features engineering\n",
    "data_train = feature_engineering(args.data_dir+'train.csv',train=True)\n",
    "with open(args.data_process_dir+'data_train.pkl','wb') as f:\n",
    "    pickle.dump(data_train,f)\n",
    "\n",
    "data_test = feature_engineering(args.data_dir+'test.csv',train=False)\n",
    "with open(args.data_process_dir+'data_test.pkl','wb') as f:\n",
    "    pickle.dump(data_test,f)\n",
    "\n",
    "\n",
    "##################features describe######################)\n",
    "print('Categary Feature number:',len(data_train['feature_cat']))\n",
    "print('numerical Feature number:',len(data_train['feature_value']))\n",
    "print('One-hot Feature number:',len(data_train['feature_onehot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31479744",
   "metadata": {
    "papermill": {
     "duration": 0.005737,
     "end_time": "2025-03-12T02:53:07.457177",
     "exception": false,
     "start_time": "2025-03-12T02:53:07.451440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Catboost Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f88c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T02:53:07.470582Z",
     "iopub.status.busy": "2025-03-12T02:53:07.470180Z",
     "iopub.status.idle": "2025-03-12T03:14:57.241571Z",
     "shell.execute_reply": "2025-03-12T03:14:57.240049Z"
    },
    "papermill": {
     "duration": 1309.780494,
     "end_time": "2025-03-12T03:14:57.243713",
     "exception": false,
     "start_time": "2025-03-12T02:53:07.463219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############fold 0\n",
      "0:\ttest: 0.6702781\tbest: 0.6702781 (0)\ttotal: 212ms\tremaining: 7m 2s\n",
      "1000:\ttest: 0.7610635\tbest: 0.7610635 (1000)\ttotal: 2m 8s\tremaining: 2m 7s\n",
      "1999:\ttest: 0.7621559\tbest: 0.7622603 (1666)\ttotal: 4m 17s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7622603455\n",
      "bestIteration = 1666\n",
      "\n",
      "############fold 1\n",
      "0:\ttest: 0.6873330\tbest: 0.6873330 (0)\ttotal: 155ms\tremaining: 5m 10s\n",
      "1000:\ttest: 0.7580478\tbest: 0.7580677 (999)\ttotal: 2m 7s\tremaining: 2m 7s\n",
      "1999:\ttest: 0.7597379\tbest: 0.7598889 (1969)\ttotal: 4m 19s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7598888507\n",
      "bestIteration = 1969\n",
      "\n",
      "############fold 2\n",
      "0:\ttest: 0.6697805\tbest: 0.6697805 (0)\ttotal: 143ms\tremaining: 4m 46s\n",
      "1000:\ttest: 0.7606891\tbest: 0.7607442 (979)\ttotal: 2m 13s\tremaining: 2m 13s\n",
      "1999:\ttest: 0.7611581\tbest: 0.7616295 (1582)\ttotal: 4m 27s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.761629532\n",
      "bestIteration = 1582\n",
      "\n",
      "############fold 3\n",
      "0:\ttest: 0.6690374\tbest: 0.6690374 (0)\ttotal: 148ms\tremaining: 4m 56s\n",
      "1000:\ttest: 0.7596601\tbest: 0.7596601 (1000)\ttotal: 2m 9s\tremaining: 2m 9s\n",
      "1999:\ttest: 0.7616068\tbest: 0.7616777 (1816)\ttotal: 4m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.761677697\n",
      "bestIteration = 1816\n",
      "\n",
      "############fold 4\n",
      "0:\ttest: 0.6894566\tbest: 0.6894566 (0)\ttotal: 142ms\tremaining: 4m 43s\n",
      "1000:\ttest: 0.7590843\tbest: 0.7590856 (998)\ttotal: 2m 8s\tremaining: 2m 8s\n",
      "1999:\ttest: 0.7607700\tbest: 0.7609789 (1896)\ttotal: 4m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7609789114\n",
      "bestIteration = 1896\n",
      "\n",
      "Over All OOF CLS-AUC: 0.7610221258516461\n"
     ]
    }
   ],
   "source": [
    "def cat_cls_seconde_process(data_file):\n",
    "    \"\"\"\n",
    "    Data process for catboost classifier\n",
    "    \"\"\"\n",
    "    with open(data_file,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for col in data['feature_cat']:\n",
    "        data['X'][col] = data['X'][col].astype('category')\n",
    "    input_columns = data['feature_cat']+data['feature_value']+data['feature_onehot']\n",
    "    data['X'] = data['X'][input_columns]\n",
    "    categories = {}\n",
    "    for col in data['X'].columns:\n",
    "        if data['X'][col].dtype=='category':\n",
    "            categories[col] = data['X'][col].cat.categories\n",
    "    return data,categories\n",
    "\n",
    "def cat_cls_train(seed,fold_n):\n",
    "    \"\"\"\n",
    "    Catboost Classifier training\n",
    "    \"\"\"\n",
    "    data,categories = cat_cls_seconde_process(args.data_process_dir+'data_train.pkl')\n",
    "    with open(args.encoder_info_dir+'./categories_cat_cls.pkl','wb') as f:\n",
    "        pickle.dump(categories,f)\n",
    "    data['cls_target'] = (data['efs']==0).astype('int')\n",
    "\n",
    "    params = {'loss_function':'Logloss',\n",
    "            'eval_metric':'AUC',\n",
    "            'cat_features': data['feature_cat'],\n",
    "            #'early_stopping_rounds': 100,\n",
    "            'verbose': 1000,\n",
    "            'random_seed': 43,\n",
    "            'depth':6, \n",
    "            'learning_rate':0.02,\n",
    "            'n_estimators':2000,\n",
    "            'l2_leaf_reg':1,\n",
    "            #'thread_count':11\n",
    "            'subsample':0.66,\n",
    "            'colsample_bylevel':0.8,\n",
    "            }\n",
    "    catbst_cls = CatBoostClassifier(**params)\n",
    "\n",
    "    sample_num = len(data['X'])\n",
    "    cls_prediction = np.zeros(sample_num)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=seed)\n",
    "    y_combine = data['efs'].astype('str')+'|'+data['X']['race_group'].astype('str')\n",
    "    for i,(train_index,eval_index) in enumerate(skf.split(data['X'],y_combine)):\n",
    "        print('############fold',i)\n",
    "\n",
    "        X_train_cls = data['X'].iloc[train_index,:]\n",
    "        y_train_cls = data['cls_target'][train_index]\n",
    "        X_eval_cls = data['X'].iloc[eval_index,:]\n",
    "        y_eval_cls = data['cls_target'][eval_index]\n",
    "\n",
    "\n",
    "        eval_set =[(X_train_cls, y_train_cls),\\\n",
    "                (X_eval_cls, y_eval_cls)]  \n",
    "        catbst_cls.fit(eval_set[0][0], eval_set[0][1], \n",
    "                eval_set=eval_set[1:2], \n",
    "                use_best_model=False\n",
    "                )\n",
    "        y_hat_cls = catbst_cls.predict_proba(X_eval_cls)[:,1]\n",
    "        cls_prediction[eval_index]=y_hat_cls\n",
    "\n",
    "        model_info = {'train_index':train_index,'eval_index':eval_index,'model':catbst_cls}\n",
    "        with open(args.model_dir+'catboost_cls_fold%s.pkl'%(i),'wb') as f:\n",
    "            pickle.dump(model_info,f)\n",
    "\n",
    "    print('Over All OOF CLS-AUC:',roc_auc_score(data['cls_target'],cls_prediction))\n",
    "\n",
    "\n",
    "cat_cls_train(888,args.fold_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a08ea",
   "metadata": {
    "papermill": {
     "duration": 0.00651,
     "end_time": "2025-03-12T03:14:57.257249",
     "exception": false,
     "start_time": "2025-03-12T03:14:57.250739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lightgbm Regressor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22158ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T03:14:57.272260Z",
     "iopub.status.busy": "2025-03-12T03:14:57.271892Z",
     "iopub.status.idle": "2025-03-12T03:33:46.895551Z",
     "shell.execute_reply": "2025-03-12T03:33:46.894204Z"
    },
    "papermill": {
     "duration": 1129.646723,
     "end_time": "2025-03-12T03:33:46.910740",
     "exception": false,
     "start_time": "2025-03-12T03:14:57.264017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############fold 0\n",
      "############fold 1\n",
      "############fold 2\n",
      "############fold 3\n",
      "############fold 4\n",
      "Over All OOF C-index where efs==1: 0.7639301534318628\n"
     ]
    }
   ],
   "source": [
    "def lgb_reg_seconde_process(data_file):\n",
    "    \"\"\"\n",
    "    Data process for lightgbm classifier\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_file,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for col in data['feature_cat']+data['feature_onehot']:\n",
    "        data['X'][col] = data['X'][col].astype('category')\n",
    "    input_columns = data['feature_cat']+data['feature_value']+data['feature_onehot']\n",
    "    data['X'] = data['X'][input_columns]\n",
    "    data['X'] = data['X'].drop(['year_hct_trans2cat','hla_match_a_high_trans2cat','hla_match_b_low_trans2cat','comorbidity_score_trans2cat'],axis=1)\n",
    "    categories = {}\n",
    "    for col in data['X'].columns:\n",
    "        if data['X'][col].dtype=='category':\n",
    "            categories[col] = data['X'][col].cat.categories\n",
    "    return data,categories\n",
    "\n",
    "def lgb_reg_train(seed,fold_n):\n",
    "    # data process\n",
    "    data,categories = lgb_reg_seconde_process(args.data_process_dir+'data_train.pkl')\n",
    "    with open(args.encoder_info_dir+'./categories_lgb_reg.pkl','wb') as f:\n",
    "        pickle.dump(categories,f)\n",
    "\n",
    "    model_param = {\n",
    "            'objective': 'regression',\n",
    "            'min_child_samples': 20,\n",
    "            'num_iterations': 10000,\n",
    "            #'num_iterations': 40000,\n",
    "            'learning_rate': 0.02,\n",
    "            'extra_trees': True,\n",
    "            'reg_lambda': 0,\n",
    "            'reg_alpha': 0,\n",
    "            'num_leaves': 128,\n",
    "            'metric': 'mae',\n",
    "            'max_depth': 6,\n",
    "            'device': 'cpu',\n",
    "            'max_bin': 128,\n",
    "            'verbose': -1,\n",
    "            'seed': 42,\n",
    "            'num_threads':11\n",
    "        }\n",
    "    lgb_reg = LGBMRegressor(**model_param)\n",
    "\n",
    "    # Create target: Grouped and normalized rank by efs as \n",
    "    efs_time_norm = data['efs_time'].copy()\n",
    "    efs_time_norm[data['efs']==1] = data['efs_time'][data['efs']==1].rank()/sum(data['efs']==1)\n",
    "    efs_time_norm[data['efs']==0] = data['efs_time'][data['efs']==0].rank()/sum(data['efs']==0)\n",
    "    data['efs_time_norm'] = efs_time_norm\n",
    "\n",
    "    # create sample weight\n",
    "    sample_weight = np.zeros(len(data['efs']))\n",
    "    sample_weight[data['efs']==1] = 0.6\n",
    "    sample_weight[data['efs']==0] = 0.4\n",
    "    data['sample_weight'] = sample_weight\n",
    "\n",
    "    sample_num = len(data['X'])\n",
    "    reg_prediction = np.zeros(sample_num)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=seed)\n",
    "    y_combine = data['efs'].astype('str')+'|'+data['X']['race_group'].astype('str')\n",
    "    for i,(train_index,eval_index) in enumerate(skf.split(data['X'],y_combine)):\n",
    "        print('############fold',i)\n",
    "        # define data\n",
    "        efs_train = data['efs'].iloc[train_index]\n",
    "        efs_eval = data['efs'].iloc[eval_index]\n",
    "\n",
    "        X_train_reg = data['X'].iloc[train_index,:]\n",
    "        X_eval_reg =  data['X'].iloc[eval_index,:]\n",
    "        # Add efs as extral feature\n",
    "        X_train_reg['efs'] = data['efs'].iloc[train_index].astype('int')  \n",
    "        X_eval_reg['efs'] = data['efs'].iloc[eval_index].astype('int')  \n",
    "        X_train_reg['efs'] = X_train_reg['efs'].astype('category').cat.set_categories([0,1])\n",
    "        X_eval_reg['efs'] = X_eval_reg['efs'].astype('category').cat.set_categories([0,1])\n",
    "        \n",
    "        y_train_reg = data['efs_time_norm'].iloc[train_index]\n",
    "        y_eval_reg = data['efs_time_norm'].iloc[eval_index]\n",
    "                \n",
    "        eval_set = [(X_train_reg, y_train_reg),\\\n",
    "                    (X_eval_reg[efs_eval==1], y_eval_reg[efs_eval==1])]    # focus metric on efs==1\n",
    "        \n",
    "        # train\n",
    "        lgb_reg.fit(eval_set[0][0], eval_set[0][1], \n",
    "                eval_set=eval_set[1:2], \n",
    "                sample_weight = data['sample_weight'][train_index],\n",
    "                #eval_metric=eval_cindex,\n",
    "                callbacks=[\n",
    "                    #lgb.early_stopping(stopping_rounds=100),\n",
    "                    #lgb.log_evaluation(period=1000),\n",
    "                    ]\n",
    "                )\n",
    "        \n",
    "        # OOF predict\n",
    "        y_hat_reg = lgb_reg.predict(X_eval_reg) \n",
    "        reg_prediction[eval_index]=y_hat_reg\n",
    "\n",
    "        # save model\n",
    "        model_info = {'train_index':train_index,'eval_index':eval_index,'model':lgb_reg}\n",
    "        with open(args.model_dir+'lightgbm_reg_fold%s.pkl'%(i),'wb') as f:\n",
    "            pickle.dump(model_info,f)\n",
    "\n",
    "    # calculate raw c-index on efs==1\n",
    "    efs_index_1 = data['efs']==1\n",
    "    c_index_overall = concordance_index(data['efs_time'][efs_index_1],reg_prediction[efs_index_1],data['efs'][efs_index_1])\n",
    "    print('Over All OOF C-index where efs==1:',c_index_overall)\n",
    "\n",
    "lgb_reg_train(888,args.fold_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8226357",
   "metadata": {
    "papermill": {
     "duration": 0.006859,
     "end_time": "2025-03-12T03:33:46.930238",
     "exception": false,
     "start_time": "2025-03-12T03:33:46.923379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merge Regressor and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dbd2a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T03:33:46.947119Z",
     "iopub.status.busy": "2025-03-12T03:33:46.946625Z",
     "iopub.status.idle": "2025-03-12T03:35:15.692309Z",
     "shell.execute_reply": "2025-03-12T03:35:15.690743Z"
    },
    "papermill": {
     "duration": 88.756908,
     "end_time": "2025-03-12T03:35:15.694476",
     "exception": false,
     "start_time": "2025-03-12T03:33:46.937568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls auc: 0.7610221258516461\n",
      "reg c-index where efs==1: 0.7639301534318628\n",
      "search best merge param\n",
      "Best merge param fold0: {'a': 2.8176971060205265, 'b': 0.9956959106905089, 'c': 0.3640555139490638}\n",
      "Best merge param fold1: {'a': 2.6882900421264573, 'b': 0.9398176725579255, 'c': 0.39208863838557917}\n",
      "Best merge param fold2: {'a': 2.8235876806812215, 'b': 0.9584395665349859, 'c': 0.38156021131269563}\n",
      "Best merge param fold3: {'a': 2.7175481124205727, 'b': 1.2154252676785475, 'c': 0.3979879581782422}\n",
      "Best merge param fold4: {'a': 3.3016546707854824, 'b': 1.0555694235271247, 'c': 0.2871540746957343}\n",
      "OOF Stratified C-index: 0.6918450031862138\n"
     ]
    }
   ],
   "source": [
    "with open(args.data_process_dir+'data_train.pkl','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "sample_num_train = len(data['X'])\n",
    "with open(args.data_process_dir+'data_test.pkl','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "sample_num_test = len(data['X'])\n",
    "#####################################分类模型#######################################\n",
    "\n",
    "def cls_predict(fold_n,test):\n",
    "    # data process\n",
    "    if test:\n",
    "        data_file = args.data_process_dir+'data_test.pkl'\n",
    "    else:\n",
    "        data_file = args.data_process_dir+'data_train.pkl'\n",
    "\n",
    "\n",
    "    data,_ = cat_cls_seconde_process(data_file)\n",
    "    with open(args.encoder_info_dir+'categories_cat_cls.pkl','rb') as f:\n",
    "        categories = pickle.load(f)\n",
    "    for col in data['X'].columns:\n",
    "        if data['X'][col].dtype=='category':\n",
    "            data['X'][col] = data['X'][col].cat.set_categories(categories[col]) \n",
    "\n",
    "\n",
    "    output = np.zeros(len(data['X']))\n",
    "    for i in range(fold_n):\n",
    "        with open(args.model_dir+f'catboost_cls_fold{i}.pkl','rb') as f:\n",
    "            model_info = pickle.load(f) \n",
    "        model = model_info['model']\n",
    "\n",
    "        if test:\n",
    "            y_hat = model.predict_proba(data['X'])[:,1]\n",
    "            output+=y_hat/fold_n\n",
    "        else:\n",
    "            eval_index = model_info['eval_index']\n",
    "            y_hat = model.predict_proba(data['X'].loc[eval_index])[:,1]            \n",
    "            output[eval_index] = y_hat\n",
    "    return output\n",
    "\n",
    "\n",
    "#####################################回归模型#######################################\n",
    "def reg_predict(fold_n,test):\n",
    "    '''\n",
    "    input:\n",
    "        model_nm: xgboost/catboost/lightgbm\n",
    "        fold_n: CV fold number when the model was training\n",
    "        test: \n",
    "            True:inferece in the test dataset\n",
    "            False: inferece the oof result in the train dataset\n",
    "    output:\n",
    "        predicted result of test data or oof\n",
    "    '''\n",
    "\n",
    "    # data process\n",
    "    if test:\n",
    "        data_file = args.data_process_dir+'data_test.pkl'\n",
    "    else:\n",
    "        data_file = args.data_process_dir+'data_train.pkl'\n",
    "    data,_ = lgb_reg_seconde_process(data_file)\n",
    "    with open(args.encoder_info_dir+'categories_lgb_reg.pkl','rb') as f:\n",
    "        categories = pickle.load(f)\n",
    "    for col in data['X'].columns:\n",
    "        if data['X'][col].dtype=='category':\n",
    "            data['X'][col] = data['X'][col].cat.set_categories(categories[col]) \n",
    "\n",
    "    data['X']['efs']=1\n",
    "    data['X']['efs'] = data['X']['efs'].astype('category').cat.set_categories([0,1])\n",
    "\n",
    "    output = np.zeros(len(data['X']))\n",
    "    for i in range(fold_n):\n",
    "        with open(args.model_dir+f'lightgbm_reg_fold{i}.pkl','rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        model = model_info['model']\n",
    "\n",
    "        if test:\n",
    "            y_hat = model.predict(data['X'])\n",
    "            output+=y_hat/fold_n\n",
    "        else:\n",
    "            eval_index = model_info['eval_index']\n",
    "            output[eval_index] = model.predict(data['X'].loc[eval_index])\n",
    "    return output\n",
    "\n",
    "###################################### Search best merge param ######################################\n",
    "\n",
    "def merge_fun(Y_HAT_REG,Y_HAT_CLS,a=2.96,b=1,c=0.52):\n",
    "    y_fun = (Y_HAT_REG>0)*c*np.abs(Y_HAT_REG)**b\n",
    "    x_fun = (Y_HAT_CLS>0)*np.abs(Y_HAT_CLS)**a\n",
    "    res = (1-y_fun)*x_fun+y_fun\n",
    "    \n",
    "    # **make sure there is enough samples**\n",
    "    res = pd.Series(res).rank()/len(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def combine_objective(trial,y_hat_reg,y_hat_cls,efs_time,efs,race_group):\n",
    "    '''\n",
    "    Objective funtion of optuna for search the best merge function params\n",
    "    input:\n",
    "        trial: optuna trial param\n",
    "        y_hat_reg: regressor prediction\n",
    "        y_hat_cls: classifier predction\n",
    "        efs_time: efs_time of train dataset\n",
    "        efs: efs of train dataset\n",
    "        race_group: race group of train dataset\n",
    "        cls_model_nm: classifier model name(set different search range for differen model)\n",
    "    output:\n",
    "        score: Stratified c-index\n",
    "    '''\n",
    "    params = {'Y_HAT_REG':y_hat_reg,\n",
    "            'Y_HAT_CLS':y_hat_cls,\n",
    "            'a':trial.suggest_uniform(\"a\", 2, 3.5),\n",
    "            'b':trial.suggest_uniform('b',0.5,1.5),  # could be setted as 1 for simplify\n",
    "            'c':trial.suggest_uniform('c',0,1),\n",
    "            }\n",
    "\n",
    "    score,var_error,metric_list = CIBMTR_score(efs_time,merge_fun(**params),efs,race_group)\n",
    "    return score\n",
    "\n",
    "\n",
    "def merge_param_fit(efs,efs_time,race_group,y_hat_reg,y_hat_cls,fold_n,seed):\n",
    "    '''\n",
    "    Find the best param for model merge functiont with optuna and K-fold cross validation, given predicted cls result and reg result\n",
    "    \n",
    "    input:\n",
    "        y_hat_reg: regressor prediction\n",
    "        y_hat_cls: classifier predction\n",
    "        efs_time: efs_time of train dataset\n",
    "        efs: efs of train dataset\n",
    "        race_group: race group of train dataset\n",
    "        cls_model_nm: classifier model name(set different search range for differen model)      \n",
    "    return:\n",
    "        result_combine: oof merged result\n",
    "        best_params: best param for merge function\n",
    "    '''\n",
    "\n",
    "    y_combine = pd.Series(efs).astype('str')+'|'+pd.Series(race_group).astype('str')\n",
    "    skf = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=seed)\n",
    "    result_combine = np.zeros(len(efs))\n",
    "    \n",
    "    best_params = []\n",
    "    for i,(train_index,eval_index) in enumerate(skf.split(efs,y_combine)):\n",
    "        study=optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: combine_objective(trial, \n",
    "                                                       y_hat_reg[train_index],\n",
    "                                                       y_hat_cls[train_index],\n",
    "                                                       efs_time[train_index],\n",
    "                                                       efs[train_index],\n",
    "                                                       race_group[train_index],\n",
    "                                                       ), \n",
    "                                                       n_trials=200)\n",
    "        \n",
    "        result_combine[eval_index]=merge_fun(y_hat_reg[eval_index],y_hat_cls[eval_index],**study.best_params)\n",
    "        best_params.append(study.best_params)\n",
    "        print(f'Best merge param fold{i}:',study.best_params)\n",
    "    return result_combine,best_params\n",
    "\n",
    "\n",
    "def search_best_merge_params():\n",
    "    with open(args.data_process_dir+'data_train.pkl','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        efs = np.array(data['efs'])\n",
    "        efs_time = np.array(data['efs_time'])\n",
    "        race_group = np.array(data['race_group'])\n",
    "\n",
    "\n",
    "    y_hat_cls=cls_predict(fold_n=args.fold_n,test=False)\n",
    "    auc = roc_auc_score(efs==0,y_hat_cls)\n",
    "    print('cls auc:',auc)\n",
    "    \n",
    "    y_hat_reg=reg_predict(fold_n=args.fold_n,test=False)\n",
    "    c_index = concordance_index(efs_time[efs==1],y_hat_reg[efs==1])\n",
    "    print(f'reg c-index where efs==1:',c_index)\n",
    "            \n",
    "\n",
    "    print(f'search best merge param')\n",
    "    # search the merge params and get the oof merge result \n",
    "    y_hat_merge,best_params = merge_param_fit(\n",
    "        efs=efs,\n",
    "        efs_time=efs_time,\n",
    "        race_group=race_group,\n",
    "        y_hat_reg=y_hat_reg,\n",
    "        y_hat_cls=y_hat_cls,\n",
    "        fold_n=5,\n",
    "        seed=888\n",
    "    )  \n",
    "\n",
    "    c_index_overall,var_error,metric_list = CIBMTR_score(efs_time,y_hat_merge,efs,race_group)\n",
    "    print(f'OOF Stratified C-index:',c_index_overall)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "merge_param=search_best_merge_params()\n",
    "\n",
    "# inference\n",
    "y_hat_cls=cls_predict(fold_n=args.fold_n,test=True)\n",
    "y_hat_reg=reg_predict(fold_n=args.fold_n,test=True)\n",
    "\n",
    "y_hat_merge = []\n",
    "for param in merge_param:\n",
    "    y_hat_merge.append(merge_fun(y_hat_reg,y_hat_cls,**param))\n",
    "y_hat_merge = np.mean(np.array(y_hat_merge).T,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56f3c7",
   "metadata": {
    "papermill": {
     "duration": 0.007392,
     "end_time": "2025-03-12T03:35:15.709935",
     "exception": false,
     "start_time": "2025-03-12T03:35:15.702543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079e8631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T03:35:15.726825Z",
     "iopub.status.busy": "2025-03-12T03:35:15.726384Z",
     "iopub.status.idle": "2025-03-12T03:35:15.744664Z",
     "shell.execute_reply": "2025-03-12T03:35:15.743419Z"
    },
    "papermill": {
     "duration": 0.028912,
     "end_time": "2025-03-12T03:35:15.746300",
     "exception": false,
     "start_time": "2025-03-12T03:35:15.717388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  prediction\n",
      "0  28800    0.333333\n",
      "1  28801    0.666667\n",
      "2  28802    0.000000\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame({'ID':data_test['ID'],'prediction':1-y_hat_merge})\n",
    "pred.to_csv(args.submission_dir+'submission.csv',index=False)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2570.186142,
   "end_time": "2025-03-12T03:35:17.282529",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-12T02:52:27.096387",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
